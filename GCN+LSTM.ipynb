{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.graph_utils import partition_graph_by_lonlat\n",
    "import networkx as nx\n",
    "from jurbey.jurbey import JURBEY\n",
    "\n",
    "with open(\"data/1558537930325.jurbey\", 'rb') as tempf:\n",
    "    g = JURBEY.load(tempf.read())\n",
    "print(g.number_of_nodes())\n",
    "g_partition = partition_graph_by_lonlat(g)\n",
    "\n",
    "A = nx.adjacency_matrix(g_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our speed data uses segment ids, but the model uses sequential indexes, based on `.nodes()`\n",
    "import math\n",
    "id_to_idx = {}\n",
    "# defaultdict won't do what you expect in Pandas\n",
    "df = pandas.read_csv(\"data/timeseries_speed_april_first_week.csv\")\n",
    "\n",
    "\n",
    "for id_ in df[\"from_node\"].unique():\n",
    "    id_to_idx[id_] = math.nan\n",
    "for id_ in df[\"to_node\"].unique():\n",
    "    id_to_idx[id_] = math.nan\n",
    "    \n",
    "for idx, id_ in enumerate(g_partition.nodes()):\n",
    "    id_to_idx[id_] = idx\n",
    "# -\n",
    "\n",
    "# Let's transform ids to indeces\n",
    "df[\"from_node_idx\"] = df.replace({\"from_node\": id_to_idx})[\"from_node\"]\n",
    "df[\"to_node_idx\"] = df.replace({\"to_node\": id_to_idx})[\"to_node\"]\n",
    "\n",
    "df.head()\n",
    "\n",
    "# ## First let's build sparse 3D data tensor\n",
    "\n",
    "# +\n",
    "import torch\n",
    "TOTAL_T_STEPS = 144\n",
    "\n",
    "\n",
    "def snapshot(t, df=df, g_partition=g_partition):\n",
    "    df_t = df[[t, \"from_node_idx\", \"to_node_idx\"]]\n",
    "    df_t = df_t.dropna()\n",
    "    row = df_t[\"from_node_idx\"].tolist()\n",
    "    col = df_t[\"to_node_idx\"].tolist()\n",
    "    data = df_t[t].tolist()\n",
    "    size = len(g_partition.nodes())  \n",
    "\n",
    "    return {\"indices\": (row, col), \"values\": data, \"shape\": (size, size)}\n",
    "\n",
    "\n",
    "# +\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def build_sparse_dataset(from_=0, to=TOTAL_T_STEPS):\n",
    "    dataset = {\"indices\": ([], [], []), \"values\": []}\n",
    "    for t in range(from_, to):\n",
    "\n",
    "        snap = snapshot(str(t))\n",
    "        dataset[\"indices\"][0].extend([t] * len(snap[\"indices\"][0]))\n",
    "        dataset[\"indices\"][1].extend(snap[\"indices\"][0])\n",
    "        dataset[\"indices\"][2].extend(snap[\"indices\"][1])\n",
    "        dataset[\"values\"].extend(snap[\"values\"])\n",
    "\n",
    "    i = torch.LongTensor(dataset[\"indices\"])\n",
    "    v = torch.FloatTensor(dataset[\"values\"])\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size((to, *snap[\"shape\"])))\n",
    "\n",
    "dataset = build_sparse_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (6162, 149). Missing in data: 818224.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of data: {df.shape}. Missing in data: {df.isnull().sum().sum()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of road segment: 6162\n",
      "original number of road segment after filtering: 213\n",
      "           0        1    2        3        4        5          6         7  \\\n",
      "4        NaN      NaN  NaN  10.4688  10.6366  10.2885    10.6175   1.75854   \n",
      "7    10.2788      NaN  NaN      NaN  10.8314      NaN    3.14074   2.07388   \n",
      "16   10.8814      NaN  NaN      NaN  10.9489  10.1905    8.48206   10.6813   \n",
      "124  10.2644      NaN  NaN  10.5666      NaN  7.37511   0.915255   10.1382   \n",
      "157      NaN      NaN  NaN      NaN  10.8314      NaN    7.53377   5.43425   \n",
      "204      NaN  10.0555  NaN      NaN      NaN      NaN  0.0954053    10.709   \n",
      "351      NaN      NaN  NaN  10.4688  10.4049  7.20193    7.68655   7.31936   \n",
      "352      NaN      NaN  NaN   10.471  10.3797  0.40066   0.149782  0.486138   \n",
      "379  10.3584      NaN  NaN      NaN  10.6964  6.91769    5.38908   10.8364   \n",
      "389      NaN      NaN  NaN  10.4688    10.38   7.0872    8.65096   10.2432   \n",
      "\n",
      "            8        9  ...      134        135      136      137      138  \\\n",
      "4      3.4044  10.2246  ...      NaN        NaN  6.09877  10.2577  10.5862   \n",
      "7    0.385263  4.10448  ...      NaN    10.3951  10.5058  10.5173      NaN   \n",
      "16    5.02823    10.55  ...      NaN        NaN  1.14562   7.3679  1.86871   \n",
      "124   1.44351  1.39497  ...  10.2675    10.9493  10.3746  5.40854      NaN   \n",
      "157   10.7573  10.3509  ...      NaN     5.1058   10.129  10.5877      NaN   \n",
      "204   2.71381  10.9534  ...  4.06558    1.87949  10.1904  4.34556  2.70244   \n",
      "351   5.89438  10.2791  ...      NaN  0.0382477  8.33314  10.3388  10.4677   \n",
      "352   1.70013   10.035  ...      NaN    10.3263  10.5539  10.3388  10.6167   \n",
      "379   7.80984  10.5734  ...  10.5366    5.21668  10.7039    10.72  10.5938   \n",
      "389   8.31396  10.2003  ...      NaN  0.0382477  8.33314  10.3062  10.5862   \n",
      "\n",
      "         139      140      141       142      143  \n",
      "4        NaN  10.6695  4.31342   10.3929  10.5183  \n",
      "7        NaN      NaN  10.1899   10.4367  4.19643  \n",
      "16   10.6086  10.3597      NaN       NaN  10.5958  \n",
      "124      NaN      NaN      NaN    10.607  10.3052  \n",
      "157  10.7772  10.7558      NaN    10.811  10.4329  \n",
      "204  10.5475  10.5027  10.5904   3.02929    10.82  \n",
      "351      NaN  10.6695   5.7017   10.4506  10.4078  \n",
      "352      NaN  10.6928  8.17036  0.921596      NaN  \n",
      "379  10.5717      NaN  7.23857    10.498    10.42  \n",
      "389  10.2744  10.6695   5.7017   10.6606  10.4951  \n",
      "\n",
      "[10 rows x 144 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv(\"data/timeseries_speed_april_first_week.csv\")\n",
    "\n",
    "\n",
    "df = df.T\n",
    "print(f\"original number of road segment: {df.shape[1]}\")\n",
    "df = df.loc[:, df.isnull().mean() < 0.5]\n",
    "print(f\"original number of road segment after filtering: {df.shape[1]}\")\n",
    "df = df.T\n",
    "df = df.loc[:, df.columns != 'Unnamed: 0']\n",
    "df = df.loc[:, df.columns != 'from_node']\n",
    "df = df.loc[:, df.columns != 'to_node']\n",
    "print(df[0:10])\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DataLoader():\n",
    "    \"\"\"A class for loading and transforming data for the lstm model\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, split):\n",
    "        i_split = int(len(dataframe) * split)\n",
    "        self.data_train = dataframe.values[:i_split]\n",
    "        self.data_test  = dataframe.values[i_split:]\n",
    "        self.len_train  = len(self.data_train)\n",
    "        self.len_test   = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "\n",
    "    def get_test_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y test data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise reduce size of the training split.\n",
    "        '''\n",
    "        data_windows = []\n",
    "        for i in range(self.len_test - seq_len):\n",
    "            data_windows.append(self.data_test[i:i+seq_len])\n",
    "\n",
    "        data_windows = np.array(data_windows).astype(float)\n",
    "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
    "\n",
    "        x = data_windows[:, :-1]\n",
    "        y = data_windows[:, -1, [0]]\n",
    "        return x,y\n",
    "\n",
    "    def get_train_data(self, seq_len, normalise):\n",
    "        '''\n",
    "        Create x, y train data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise use generate_training_window() method.\n",
    "        '''\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(self.len_train - seq_len):\n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "        return np.array(data_x, dtype=np.float32), np.array(data_y, dtype=np.float32)\n",
    "\n",
    "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
    "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
    "        i = 0\n",
    "        while i < (self.len_train - seq_len):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for b in range(batch_size):\n",
    "                if i >= (self.len_train - seq_len):\n",
    "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    i = 0\n",
    "                x, y = self._next_window(i, seq_len, normalise)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                i += 1\n",
    "            yield np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.float32)\n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        '''Generates the next data window from the given index location i'''\n",
    "        window = self.data_train[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1,:]\n",
    "        y = window[-1:,:]\n",
    "        print(x.shape)\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        '''Normalise window with a base value of zero'''\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "data = DataLoader(df, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(29, 213)\n",
      "(213, 85, 1)\n",
      "(213, 85, 29)\n"
     ]
    }
   ],
   "source": [
    "x, y = data.get_train_data(seq_len=30, normalise=False)\n",
    "x = np.swapaxes(x,0,2)\n",
    "y = np.swapaxes(y,0,2)\n",
    "x = np.swapaxes(x,1,2)\n",
    "y = np.swapaxes(y,1,2)\n",
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tgcn.temporal_spatial_model import TGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan        nan        nan ...        nan 10.559644         nan]\n",
      " [       nan        nan 10.468811  ... 10.559644         nan  1.6303675]\n",
      " [       nan 10.468811  10.6366205 ...        nan  1.6303675        nan]\n",
      " ...\n",
      " [ 1.8972399 10.166076  10.721033  ...  4.6672025        nan 10.232055 ]\n",
      " [10.166076  10.721033         nan ...        nan 10.232055  10.66193  ]\n",
      " [10.721033         nan        nan ... 10.232055  10.66193   10.476906 ]]\n",
      "[[ 1.6303675 ]\n",
      " [        nan]\n",
      " [10.732706  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [ 6.3112288 ]\n",
      " [        nan]\n",
      " [10.337724  ]\n",
      " [ 1.3691394 ]\n",
      " [10.610737  ]\n",
      " [        nan]\n",
      " [10.888935  ]\n",
      " [10.657227  ]\n",
      " [        nan]\n",
      " [10.035851  ]\n",
      " [10.17424   ]\n",
      " [ 0.40861756]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.202348  ]\n",
      " [10.224177  ]\n",
      " [10.866926  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [ 1.8014525 ]\n",
      " [ 5.4981275 ]\n",
      " [10.538274  ]\n",
      " [10.052774  ]\n",
      " [        nan]\n",
      " [10.921742  ]\n",
      " [10.541224  ]\n",
      " [10.154606  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.297311  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.612251  ]\n",
      " [10.6901455 ]\n",
      " [ 1.8972399 ]\n",
      " [10.166076  ]\n",
      " [10.721033  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [ 7.2562065 ]\n",
      " [10.531919  ]\n",
      " [ 0.15242068]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.921657  ]\n",
      " [10.761448  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.233399  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.516951  ]\n",
      " [10.059192  ]\n",
      " [ 1.732224  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [10.245175  ]\n",
      " [ 4.6672025 ]\n",
      " [        nan]\n",
      " [10.232055  ]\n",
      " [10.66193   ]\n",
      " [10.476906  ]\n",
      " [10.513797  ]]\n",
      "torch.Size([213, 85, 29])\n",
      "torch.Size([213, 85, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(y[0])\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "print(x.size())\n",
    "print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False, used: False\n"
     ]
    }
   ],
   "source": [
    "from test_tube import Experiment \n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "\n",
    "# PyTorch summarywriter with a few bells and whistles    \n",
    "exp = Experiment(save_dir=os.getcwd())\n",
    "\n",
    "# train on cpu using only 10% of the data (for demo purposes)\n",
    "# pass in experiment for automatic tensorboard logging.    \n",
    "trainer = Trainer(experiment=exp, max_nb_epochs=100, train_percent_check=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(x)\n",
    "train_y = torch.from_numpy(y)\n",
    "train=torch.zeros(len(train_x),213,look_back+1)\n",
    "train[:,:,:look_back]=train_x\n",
    "train[:,:,look_back:(look_back+1)]=train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name    Type  Params\n",
      "0    lstm    LSTM   16512\n",
      "1  linear  Linear      33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguytu3/Work/speed-imputation/src/tgcn/layers/lstm.py:34: UserWarning: Using a target size (torch.Size([10, 85, 1])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return {'loss': F.mse_loss(y_hat, y)}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (85) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-e74a1be5fcfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLSTMs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_tng_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mrun_tng_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_tng_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0mearly_stop_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_result\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_tng_batch\u001b[0;34m(self, data_batch, batch_nb)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_specific_tqdm_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tng_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;31m# track metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__tng_forward\u001b[0;34m(self, data_batch, batch_nb, opt_idx)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/speed-imputation/src/tgcn/layers/lstm.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2187\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (85) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import imp  \n",
    "import src.tgcn.layers.lstm as l\n",
    "l = imp.reload(l)\n",
    "LSTMs = l.LSTMs\n",
    "model = LSTMs(input_dim=29, hidden_dim=32, data=train)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
