{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.graph_utils import partition_graph_by_lonlat\n",
    "import networkx as nx\n",
    "from jurbey.jurbey import JURBEY\n",
    "\n",
    "with open(\"../data/berlin.jurbey\", 'rb') as tempf:\n",
    "    g = JURBEY.load(tempf.read())\n",
    "print(g.number_of_nodes())\n",
    "g_partition = partition_graph_by_lonlat(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to edge-based graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "L = nx.line_graph(nx.DiGraph(g_partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arc(arcType=<ArcType.LANE_STRAIGHT: 'LANE_STRAIGHT'>, roadClass=<RoadClass.MajorRoad: 2>, roadAccessibility=<RoadAccessibility.NoRestriction: 1>, metadata={'bicycle': 'no', 'highway': 'primary', 'lanes': '4', 'lit': 'yes', 'maxspeed': '50', 'name': 'Bismarckstra√üe', 'oneway': 'yes', 'postal_code': '10625', 'ref': 'B 2;B 5', 'surface': 'asphalt', 'turn:lanes': 'through|through|through;right|right'}, signs=[], vehicleAccessibility=[], geometry=[GeoCoordinates(lon=13.3207077, lat=52.5123944, alt=nan), GeoCoordinates(lon=13.3207877, lat=52.5123711, alt=nan)])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = list(L.nodes())\n",
    "g_partition[nodes[10][0]][nodes[10][1]]['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract dynamic (speed) + static features from nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "ienc = OrdinalEncoder()\n",
    "scaler = StandardScaler()\n",
    "def arc_features(arc):\n",
    "    arc = g_partition[arc[0]][arc[1]]\n",
    "    return [\n",
    "        arc['data'].metadata['highway'],\n",
    "        arc['data'].metadata.get('surface', 'no_sur'),\n",
    "        arc['data'].roadClass.name\n",
    "    ],  [float(arc['data'].metadata.get('maxspeed', '50')), \n",
    "        int(arc['data'].metadata.get('lanes', '1'))]\n",
    "def construct_features():\n",
    "    data = list()\n",
    "    data_ord = list()\n",
    "    for node in L.nodes:\n",
    "        data.append(arc_features(node)[0])\n",
    "        data_ord.append(arc_features(node)[1])\n",
    "    return enc.fit_transform(data), ienc.fit_transform(data_ord)\n",
    "    \n",
    "x, y = construct_features()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['access_ramp', 'corridor', 'living_street', 'platform', 'primary',\n",
       "        'residential', 'secondary', 'secondary_link', 'service',\n",
       "        'tertiary', 'tertiary_link', 'unclassified'], dtype=object),\n",
       " array(['asphalt', 'cobblestone', 'cobblestone:flattened', 'concrete',\n",
       "        'concrete:plates', 'grass_paver', 'no_sur', 'paved',\n",
       "        'paving_stones', 'sett'], dtype=object),\n",
       " array(['DirtRoad', 'LocalRoad', 'MajorRoad'], dtype=object)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ienc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6163, 25)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6163x25 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18489 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess adjacency matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = nx.to_scipy_sparse_matrix(L, format=\"coo\")\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "                                    \n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "                                    \n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "                                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6163, 6163])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 16, 124, 157, 204, 351, 352, 379, 389, 430, 487, 492, 513, 539, 551, 568, 569, 587, 613, 630, 639, 649, 675, 680, 698, 711, 713, 780, 782, 784, 865, 1037, 1062, 1070, 1079, 1080, 1088, 1091, 1125, 1206, 1215, 1217, 1238, 1241, 1397, 1400, 1450, 1517, 1550, 1556, 1560, 1564, 1574, 1703, 1733, 1756, 1804, 1805, 1806, 1807, 1819, 1824, 1853, 1907, 1917, 1964, 1996, 1999, 2003, 2007, 2029, 2030, 2033, 2036, 2037, 2040, 2042, 2098, 2099, 2102, 2104, 2116, 2119, 2123, 2128, 2137, 2143, 2210, 2261, 2281, 2284, 2289, 2358, 2382, 2387, 2394, 2406, 2411, 2412, 2473, 2487, 2550, 2568, 2604, 2605, 2649, 2657, 2674, 2677, 2738, 2753, 2766, 2788, 2794, 2809, 2851, 2853, 2869, 2871, 2917, 2925, 2956, 3081, 3086, 3087, 3088, 3090, 3117, 3120, 3132, 3223, 3264, 3450, 3458, 3459, 3530, 3577, 3587, 3627, 3629, 3645, 3728, 3792, 3870, 3974, 3984, 3993, 4006, 4064, 4065, 4083, 4084, 4303, 4445, 4448, 4550, 4551, 4572, 4602, 4655, 4673, 4688, 4698, 4700, 4704, 4798, 4814, 4866, 4889, 4953, 4967, 4992, 5035, 5122, 5149, 5244, 5285, 5357, 5391, 5400, 5476, 5479, 5484, 5485, 5516, 5525, 5528, 5529, 5611, 5688, 5703, 5706, 5721, 5752, 5823, 5824, 5825, 5826, 5827, 5828, 5834, 5858, 5953, 5956, 5964, 5970, 5981, 5988, 6002, 6004, 6121, 6157]\n"
     ]
    }
   ],
   "source": [
    "#Our speed data uses segment ids, but the model uses sequential indexes, based on `.nodes()`\n",
    "import math\n",
    "id_to_idx = {}\n",
    "# defaultdict won't do what you expect in Pandas\n",
    "df = pandas.read_csv(\"../data/timeseries_speed_april_first_week.csv\")\n",
    "df = df.T\n",
    "l = (df.isnull().mean() < 0.5).tolist()\n",
    "\n",
    "indices = [i for i, x in enumerate(l) if x == True]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1          2          3          4          5  \\\n",
      "0  10.854457  10.854457  10.854457  10.854457   5.680978   5.680978   \n",
      "1  10.854457  10.854457  10.854457  10.854457   5.680978   5.680978   \n",
      "2  10.854457  10.854457  10.854457  10.854457  10.486210  10.486210   \n",
      "3  10.468811  10.468811  10.468811  10.468811  10.486210  10.486210   \n",
      "4  10.468811  10.468811  10.468811  10.468811  10.636621  10.288534   \n",
      "5  10.468811  10.468811  10.468811  10.468811  10.636621  10.288534   \n",
      "6  10.854457  10.854457  10.854457  10.854457  10.831409   5.680978   \n",
      "7  10.278846  10.854457  10.854457  10.854457  10.831409   5.680978   \n",
      "8  10.278846  10.854457  10.854457  10.854457  10.831409  10.298252   \n",
      "9  10.278846  10.854457  10.854457  10.854457  10.831409  10.298252   \n",
      "\n",
      "           6          7          8          9  ...        136        137  \\\n",
      "0   7.036838   1.499997  10.088634  10.587459  ...  10.581123  10.850843   \n",
      "1   7.036838   1.499997  10.088634  10.587459  ...  10.581123  10.850843   \n",
      "2   7.036838   1.499997  10.937218  10.587459  ...  10.581123  10.850843   \n",
      "3  10.617513   1.758539  10.937218  10.224568  ...   6.098767  10.392901   \n",
      "4  10.617513   1.758539   3.404401  10.224568  ...   6.098767  10.257657   \n",
      "5  10.286999   1.758539   3.404401  10.833570  ...  10.537584  10.286999   \n",
      "6   7.036838   1.499997   5.044317  10.587459  ...   0.961866  10.286999   \n",
      "7   3.140737   2.073876   0.385263   4.104480  ...  10.505827  10.517314   \n",
      "8   1.220956  10.298252   0.385263   4.104480  ...  10.505827  10.517314   \n",
      "9   1.220956  10.298252   0.385263   4.104480  ...  10.505827  10.517314   \n",
      "\n",
      "         138        139        140        141        142        143  \\\n",
      "0  10.850843  10.850843  10.850843  10.850843  10.850843  10.036408   \n",
      "1  10.850843  10.850843  10.850843  10.850843  10.850843  10.036408   \n",
      "2  10.850843  10.850843  10.850843  10.850843  10.850843  10.036408   \n",
      "3  10.392901  10.392901  10.392901  10.392901  10.392901  10.518310   \n",
      "4  10.586166  10.669476  10.669476   4.313417  10.392901  10.518310   \n",
      "5  10.586166  10.669476  10.669476   7.238553  10.497979  10.491733   \n",
      "6  10.586166  10.669476  10.669476   7.238553  10.850843  10.036408   \n",
      "7  10.586166  10.669476  10.669476  10.189923  10.436750   4.196431   \n",
      "8  10.586166  10.669476  10.669476  10.189923  10.436750   4.196431   \n",
      "9  10.586166  10.669476  10.669476  10.189923  10.436750   4.196431   \n",
      "\n",
      "    from_node     to_node  \n",
      "0   628154368  1023689595  \n",
      "1   628154368  1023689595  \n",
      "2   527147009    27537239  \n",
      "3   527147009    26908815  \n",
      "4   628154370  3804638178  \n",
      "5   628154372    26938222  \n",
      "6   628154375  1560866145  \n",
      "7  5791596551  1321327852  \n",
      "8  5791621141  4782446443  \n",
      "9  5791621141    26875019  \n",
      "\n",
      "[10 rows x 146 columns]\n"
     ]
    }
   ],
   "source": [
    "id_to_idx = {}\n",
    "\n",
    "for idx, id_ in enumerate(L.nodes()):\n",
    "    id_to_idx[id_] = idx\n",
    "df = df.T\n",
    "df = df.loc[:, df.columns != 'Unnamed: 0']\n",
    "\n",
    "df2 = df['from_node']\n",
    "df3 = df['to_node']\n",
    "\n",
    "df_filled = df.loc[:, df.columns != 'from_node']\n",
    "df_filled = df.loc[:, df.columns != 'to_node']\n",
    "\n",
    "\n",
    "df_filled = df_filled.T\n",
    "for column in df_filled:\n",
    "    df_filled[column] = pandas.to_numeric(df_filled[column])\n",
    "\n",
    "df_filled = df_filled.interpolate(method='nearest', axis=1)\n",
    "\n",
    "df_filled = df_filled.fillna(method='backfill')\n",
    "df_filled = df_filled.T\n",
    "df_filled['from_node'] = df2\n",
    "df_filled['to_node'] = df3\n",
    "\n",
    "print(df_filled[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>from_node</th>\n",
       "      <th>to_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.68098</td>\n",
       "      <td>7.03684</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10.0886</td>\n",
       "      <td>10.5875</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8508</td>\n",
       "      <td>10.0364</td>\n",
       "      <td>628154368</td>\n",
       "      <td>1023689595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.68098</td>\n",
       "      <td>7.03684</td>\n",
       "      <td>1.5</td>\n",
       "      <td>10.0886</td>\n",
       "      <td>10.5875</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5811</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8508</td>\n",
       "      <td>10.0364</td>\n",
       "      <td>628154368</td>\n",
       "      <td>1023689595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.4862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.9372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>527147009</td>\n",
       "      <td>27537239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>527147009</td>\n",
       "      <td>26908815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.4688</td>\n",
       "      <td>10.6366</td>\n",
       "      <td>10.2885</td>\n",
       "      <td>10.6175</td>\n",
       "      <td>1.75854</td>\n",
       "      <td>3.4044</td>\n",
       "      <td>10.2246</td>\n",
       "      <td>...</td>\n",
       "      <td>6.09877</td>\n",
       "      <td>10.2577</td>\n",
       "      <td>10.5862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.6695</td>\n",
       "      <td>4.31342</td>\n",
       "      <td>10.3929</td>\n",
       "      <td>10.5183</td>\n",
       "      <td>628154370</td>\n",
       "      <td>3804638178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8336</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5376</td>\n",
       "      <td>10.287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.23855</td>\n",
       "      <td>10.498</td>\n",
       "      <td>10.4917</td>\n",
       "      <td>628154372</td>\n",
       "      <td>26938222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.68098</td>\n",
       "      <td>7.03684</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.04432</td>\n",
       "      <td>10.5875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8508</td>\n",
       "      <td>10.0364</td>\n",
       "      <td>628154375</td>\n",
       "      <td>1560866145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.2788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.8314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.14074</td>\n",
       "      <td>2.07388</td>\n",
       "      <td>0.385263</td>\n",
       "      <td>4.10448</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5058</td>\n",
       "      <td>10.5173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1899</td>\n",
       "      <td>10.4367</td>\n",
       "      <td>4.19643</td>\n",
       "      <td>5791596551</td>\n",
       "      <td>1321327852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.2983</td>\n",
       "      <td>1.22096</td>\n",
       "      <td>10.2983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5791621141</td>\n",
       "      <td>4782446443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5791621141</td>\n",
       "      <td>26875019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2        3        4        5        6        7         8  \\\n",
       "0      NaN  NaN  NaN  10.8545      NaN  5.68098  7.03684      1.5   10.0886   \n",
       "1      NaN  NaN  NaN  10.8545      NaN  5.68098  7.03684      1.5   10.0886   \n",
       "2      NaN  NaN  NaN      NaN      NaN  10.4862      NaN      NaN   10.9372   \n",
       "3      NaN  NaN  NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "4      NaN  NaN  NaN  10.4688  10.6366  10.2885  10.6175  1.75854    3.4044   \n",
       "5      NaN  NaN  NaN      NaN      NaN      NaN   10.287      NaN       NaN   \n",
       "6      NaN  NaN  NaN  10.8545      NaN  5.68098  7.03684      1.5   5.04432   \n",
       "7  10.2788  NaN  NaN      NaN  10.8314      NaN  3.14074  2.07388  0.385263   \n",
       "8      NaN  NaN  NaN      NaN      NaN  10.2983  1.22096  10.2983       NaN   \n",
       "9      NaN  NaN  NaN      NaN      NaN      NaN      NaN      NaN       NaN   \n",
       "\n",
       "         9  ...       136      137      138  139      140      141      142  \\\n",
       "0  10.5875  ...   10.5811      NaN      NaN  NaN      NaN      NaN  10.8508   \n",
       "1  10.5875  ...   10.5811      NaN      NaN  NaN      NaN      NaN  10.8508   \n",
       "2      NaN  ...       NaN      NaN      NaN  NaN      NaN      NaN      NaN   \n",
       "3      NaN  ...       NaN      NaN      NaN  NaN      NaN      NaN      NaN   \n",
       "4  10.2246  ...   6.09877  10.2577  10.5862  NaN  10.6695  4.31342  10.3929   \n",
       "5  10.8336  ...   10.5376   10.287      NaN  NaN      NaN  7.23855   10.498   \n",
       "6  10.5875  ...  0.961866      NaN      NaN  NaN      NaN      NaN  10.8508   \n",
       "7  4.10448  ...   10.5058  10.5173      NaN  NaN      NaN  10.1899  10.4367   \n",
       "8      NaN  ...       NaN      NaN      NaN  NaN      NaN      NaN      NaN   \n",
       "9      NaN  ...       NaN      NaN      NaN  NaN      NaN      NaN      NaN   \n",
       "\n",
       "       143   from_node     to_node  \n",
       "0  10.0364   628154368  1023689595  \n",
       "1  10.0364   628154368  1023689595  \n",
       "2      NaN   527147009    27537239  \n",
       "3      NaN   527147009    26908815  \n",
       "4  10.5183   628154370  3804638178  \n",
       "5  10.4917   628154372    26938222  \n",
       "6  10.0364   628154375  1560866145  \n",
       "7  4.19643  5791596551  1321327852  \n",
       "8      NaN  5791621141  4782446443  \n",
       "9      NaN  5791621141    26875019  \n",
       "\n",
       "[10 rows x 146 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create rolling window tensor dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.sparse\n",
    "TOTAL_T_STEPS = 144\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "ienc = OrdinalEncoder()\n",
    "    \n",
    "def build_dataset_to_numpy_tensor(from_=0, to=TOTAL_T_STEPS, df=None):\n",
    "    dataset = list()\n",
    "    for t in range(from_, to):\n",
    "        cat_features_at_t = [['primary', 'asphalt', 'MajorRoad']] * len(L.nodes)\n",
    "        ord_features_at_t = [[50.0, 4]] * len(L.nodes)\n",
    "        speed_features_at_t = [50] * len(L.nodes) \n",
    "        speed_is_nan_feature = [1] * len(L.nodes)\n",
    "        for _, row in df.iterrows():\n",
    "\n",
    "            arc = (row['from_node'], row['to_node'])\n",
    "            cat_features_at_t[id_to_idx[arc]], ord_features_at_t[id_to_idx[arc]]  = arc_features(arc)\n",
    "            speed_features_at_t[id_to_idx[arc]] = row[str(t)]\n",
    "            if np.isnan(row[str(t)]): \n",
    "                speed_is_nan_feature[id_to_idx[arc]] = 0\n",
    "        dataset.append(np.concatenate([scaler.fit_transform(np.array(speed_features_at_t).reshape(-1, 1)), \n",
    "                                       np.array(speed_is_nan_feature).reshape(-1, 1), \n",
    "                                       ienc.fit_transform(ord_features_at_t),\n",
    "                                       enc.fit_transform(cat_features_at_t).toarray()], axis=1))\n",
    "    return np.stack(dataset, axis=0)\n",
    "\n",
    "Y = build_dataset_to_numpy_tensor(df=df)\n",
    "Y_filled = build_dataset_to_numpy_tensor(df=df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 6163, 29)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the sparse tensor dataset\n",
    "# torch.save(X.indices(), \"../data/dataset_st_indices.pt\", pickle_protocol=4)\n",
    "# torch.save(X.values(), \"../data/dataset_st_values.pt\", pickle_protocol=4)\n",
    "# torch.save(X.size(), \"../data/dataset_st_size.pt\", pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6163, 29, 144)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.moveaxis(Y, source=(0,1,2), destination=(2,0,1))\n",
    "X_filled = np.moveaxis(Y_filled, source=(0,1,2), destination=(2,0,1))\n",
    "\n",
    "\n",
    "# num_vertices, num_features, num_timesteps\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6163, 29, 144)\n",
      "torch.Size([6163, 29, 144])\n"
     ]
    }
   ],
   "source": [
    "# Build mask tensor\n",
    "X_masked = torch.where(torch.isnan(torch.from_numpy(X)), torch.tensor([0]), torch.tensor([1]))\n",
    "X_masked = X_masked.bool()\n",
    "print(X.shape)\n",
    "print(X_masked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(X, num_timesteps_input, num_timesteps_output):\n",
    "    \"\"\"\n",
    "    Takes node features for the graph and divides them into multiple samples\n",
    "    along the time-axis by sliding a window of size (num_timesteps_input+\n",
    "    num_timesteps_output) across it in steps of 1.\n",
    "    :param X: Node features of shape (num_vertices, num_features,\n",
    "    num_timesteps)\n",
    "    :return:\n",
    "        - Node features divided into multiple samples. Shape is\n",
    "          (num_samples, num_vertices, num_features, num_timesteps_input).\n",
    "        - Node targets for the samples. Shape is\n",
    "          (num_samples, num_vertices, num_features, num_timesteps_output).\n",
    "    \"\"\"\n",
    "    # Generate the beginning index and the ending index of a sample, which\n",
    "    # contains (num_points_for_training + num_points_for_predicting) points\n",
    "    indices = [(i, i + (num_timesteps_input + num_timesteps_output)) for i\n",
    "               in range(X.shape[2] - (\n",
    "                num_timesteps_input + num_timesteps_output) + 1)]\n",
    "\n",
    "    # Save samples\n",
    "    features, target = [], []\n",
    "    for i, j in indices:\n",
    "        features.append(\n",
    "            X[:, :, i: i + num_timesteps_input])\n",
    "        target.append(X[:, :, i + num_timesteps_input: j])\n",
    "\n",
    "    return torch.from_numpy(np.array(features)).permute(0,1,3,2), \\\n",
    "           torch.from_numpy(np.array(target)).permute(0,1,3,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_dataset_concat(X, X_masked, num_timesteps_input, num_timesteps_output):\n",
    "    \"\"\"\n",
    "    Takes node features for the graph and divides them into multiple samples\n",
    "    along the time-axis by sliding a window of size (num_timesteps_input+\n",
    "    num_timesteps_output) across it in steps of 1.\n",
    "    :param X: Node features of shape (num_vertices, num_features,\n",
    "    num_timesteps)\n",
    "    :return:\n",
    "        - Node data (features + labels) divided into multiple samples. Shape is\n",
    "          (num_samples, num_vertices, num_features, num_timesteps_input).\n",
    "        \n",
    "    \"\"\"\n",
    "    # Generate the beginning index and the ending index of a sample, which\n",
    "    # contains (num_points_for_training + num_points_for_predicting) points\n",
    "    indices = [(i, i + (num_timesteps_input + num_timesteps_output)) for i\n",
    "               in range(X.shape[2] - (\n",
    "                num_timesteps_input + num_timesteps_output) + 1)]\n",
    "\n",
    "    # Save samples\n",
    "    features, target = [], []\n",
    "    mask = []\n",
    "    for i, j in indices:\n",
    "        features.append(X[:, :, i: i + num_timesteps_input])\n",
    "        target.append(X[:, 0, i + num_timesteps_input: j])\n",
    "        mask.append(X_masked[:, 0, i + num_timesteps_input: j])\n",
    "\n",
    "    return torch.from_numpy(np.array(features)), torch.from_numpy(np.array(target)),torch.stack(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training: torch.Size([91, 6163, 29, 9]), torch.Size([91, 6163, 1]), torch.Size([91, 6163, 1])\n",
      "shape of validation: torch.Size([20, 6163, 29, 9]), torch.Size([20, 6163, 1]), torch.Size([20, 6163, 1])\n",
      "shape of testing: torch.Size([6, 6163, 29, 9]), torch.Size([6, 6163, 1]), torch.Size([6, 6163, 1])\n"
     ]
    }
   ],
   "source": [
    "# training, validation, testing : 0.7, 0.1, 0.2\n",
    "split_line1 = int(X.shape[2] * 0.7)\n",
    "split_line2 = int(X.shape[2] * 0.9)\n",
    "train_original_data = X_filled[:, :, :split_line1]\n",
    "val_original_data = X[:, :, split_line1:split_line2]\n",
    "test_original_data = X[:, :, split_line2:]\n",
    "\n",
    "train_mask = X_masked[:, :, :split_line1]\n",
    "valid_mask = X_masked[:, :, split_line1:split_line2]\n",
    "test_mask = X_masked[:, :, split_line2:]\n",
    "\n",
    "\n",
    "look_back = 9\n",
    "look_ahead = 1\n",
    "# num_samples, num_nodes, num_timesteps, num_features\n",
    "\n",
    "training_data, training_target, train_mask = generate_dataset_concat(train_original_data, train_mask,\n",
    "                                                       num_timesteps_input=look_back,\n",
    "                                                       num_timesteps_output=look_ahead)\n",
    "valid_data, valid_target, valid_mask = generate_dataset_concat(val_original_data, valid_mask,\n",
    "                                             num_timesteps_input=look_back,\n",
    "                                             num_timesteps_output=look_ahead)\n",
    "test_data, test_target, test_mask = generate_dataset_concat(test_original_data, test_mask,\n",
    "                                               num_timesteps_input=look_back,\n",
    "                                               num_timesteps_output=look_ahead)\n",
    "\n",
    "print(f\"shape of training: {training_data.shape}, {training_target.shape}, {train_mask.shape}\")\n",
    "print(f\"shape of validation: {valid_data.shape}, {valid_target.shape}, {valid_mask.shape}\")\n",
    "print(f\"shape of testing: {test_data.shape}, {test_target.shape}, {test_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6163, 1])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask[1, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('data.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"train\", data=training_data)\n",
    "    f.create_dataset(\"valid\", data=valid_data)\n",
    "    f.create_dataset(\"test\", data=test_data)\n",
    "with h5py.File('target.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"train\", data=training_target)\n",
    "    f.create_dataset(\"valid\", data=valid_target)\n",
    "    f.create_dataset(\"test\", data=test_target)\n",
    "with h5py.File('mask.hdf5', 'w') as f:\n",
    "    f.create_dataset(\"train\", data=train_mask)\n",
    "    f.create_dataset(\"valid\", data=valid_mask)\n",
    "    f.create_dataset(\"test\", data=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 6163, 36, 9])\n",
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        ...,\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n",
      "tensor([[0.2421],\n",
      "        [0.2421],\n",
      "        [   nan],\n",
      "        ...,\n",
      "        [   nan],\n",
      "        [   nan],\n",
      "        [   nan]], dtype=torch.float64)\n",
      "tensor([ 0.4326,  0.4197,  0.3346,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,  0.3471,  0.3471,     nan,\n",
      "            nan,     nan,     nan,  0.3885,     nan,     nan,     nan,     nan,\n",
      "         0.4559,  0.3665,  0.3665,     nan,     nan,  0.4451,  0.3479,     nan,\n",
      "            nan,  0.3885,  0.3297,     nan,     nan,     nan,  0.3885,     nan,\n",
      "            nan,     nan,     nan,  0.2709,  0.3003,  0.3585,  0.4485,  0.4485,\n",
      "            nan,     nan,     nan,  0.3577,  0.3252,  0.3003,     nan,     nan,\n",
      "         0.2708,  0.2708,     nan,     nan,     nan,     nan,     nan,  0.3227,\n",
      "        -0.6791,  0.4455,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.4414,  0.4414,  0.3410,  0.3230,  0.2885,  0.3463,\n",
      "            nan,  0.2421,  0.4455,  0.4044,     nan,     nan,  0.3471,  0.4455,\n",
      "            nan,  0.3665,  0.3665,  0.4414,  0.4414,  0.2384,     nan,     nan,\n",
      "            nan,     nan,  0.4455,     nan,     nan,     nan,  0.2906,  0.2906,\n",
      "         0.2906,     nan,     nan,     nan,  0.4455,     nan,     nan,  0.4044,\n",
      "            nan,     nan, -1.6810,  0.4455,  0.4455,  0.4455,     nan,     nan,\n",
      "            nan,  0.2472,     nan,     nan,  0.2906,  0.2906,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,  0.3370,     nan,\n",
      "            nan,     nan,     nan,     nan,  0.2709,  0.2709,  0.2709,     nan,\n",
      "        -2.0998,     nan,     nan,     nan,     nan,     nan,  0.4638,  0.4638,\n",
      "            nan,  0.3253,     nan,     nan,  0.2421, -1.9640,     nan,     nan,\n",
      "         0.3230,  0.4378,  0.3230,  0.4638,  0.4638,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.3230,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.4638,  0.4638,     nan,     nan,     nan,     nan,\n",
      "            nan, -1.7941,     nan, -0.6847, -0.6847,     nan, -0.6847,  0.2472,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "         0.2906,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,  0.3230,  0.3230,  0.3230, -2.1130,     nan,     nan,     nan,\n",
      "            nan,  0.3539,  0.3929,  0.3253,  0.3253,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,  0.2709,  0.2709,  0.4455,\n",
      "         0.3230, -1.9577,     nan,  0.2609,  0.3884,  0.4414,  0.4414,  0.4414,\n",
      "            nan,     nan, -1.7177,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,  0.4414,  0.4414,     nan, -1.7635,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.4197,  0.4197,     nan,     nan,  0.4197,     nan,\n",
      "         0.2709,  0.2709,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "         0.4530, -1.3790,     nan, -0.6847,     nan,  0.2826,     nan,     nan,\n",
      "            nan,  0.2826,  0.2826,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.3572,     nan,  9.6250,     nan,     nan,  0.3230,\n",
      "            nan,  0.3230,     nan,  0.2826,     nan,  0.2709,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "         0.3435,  0.4485,  0.3410,  0.4044,  0.4044,     nan, -2.0183,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "         0.4414,     nan,     nan,     nan,     nan,     nan,  0.2421,  0.4455,\n",
      "         0.4455,     nan,     nan, -1.6962,     nan, -1.6695,     nan,     nan,\n",
      "         0.2906,     nan,     nan,  0.4414,     nan,     nan,     nan,  0.3665,\n",
      "         0.3665,     nan,     nan,     nan,     nan,     nan, -1.6021,     nan,\n",
      "            nan,     nan,     nan,     nan, -1.8546, -1.8546,     nan,     nan,\n",
      "            nan,     nan,  0.4455,     nan,  0.4044,  0.4455,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,  0.3185, -1.7632,     nan,  0.3370,\n",
      "         0.2906,     nan,     nan,     nan,     nan,     nan,  0.3665,  0.4638,\n",
      "         0.2708,     nan,     nan, -1.9575,     nan, -1.3222,     nan,     nan,\n",
      "         0.4635,  0.3539,     nan,  0.4638,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan, -0.8623,     nan,\n",
      "            nan,     nan,     nan,  0.2709,  0.2709,  0.3230,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,  0.3665,  0.3665,     nan,\n",
      "         0.3058, -1.6493,  0.3665,  0.3665,  0.3665,  0.3231,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.3253,     nan,  0.4197,  0.4197,     nan,  0.4197,\n",
      "         0.4638,     nan,  0.2708,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,  0.3253,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,  9.6250,     nan,     nan,     nan,     nan,  0.2906,\n",
      "            nan,     nan,     nan,     nan,  0.4638,     nan,  0.2788,     nan,\n",
      "            nan,     nan,     nan,  0.3253, -0.7956,  0.3253, -0.4875,     nan,\n",
      "            nan,  0.3665,     nan,     nan,     nan,     nan,  0.2708,     nan,\n",
      "         0.2708,     nan,     nan,     nan,  0.2507,     nan,  0.4638,     nan,\n",
      "         0.2708,  0.4638,  0.2708,     nan,  0.3187,  0.2708,  0.2708,     nan,\n",
      "            nan,     nan,  0.4626, -1.3480,     nan,  0.2708,  0.2708,     nan,\n",
      "         0.2922,  0.2472,     nan,     nan,  0.2421,     nan,  0.3253,     nan,\n",
      "         0.3885,     nan,     nan,  0.4638,  0.2708,     nan,     nan,  0.3685,\n",
      "         0.3665,     nan,     nan,  0.4378,     nan,     nan, -1.9408,     nan,\n",
      "            nan, -1.9526,  0.4555,     nan,     nan,     nan,     nan,  0.4638,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data['train'] = training_data\n",
    "data['valid'] = valid_data\n",
    "data['test'] = test_data\n",
    "\n",
    "target['train'] = training_target\n",
    "target['valid'] = valid_target\n",
    "target['test'] = test_target\n",
    "\n",
    "mask = {}\n",
    "mask['train'] = train_mask\n",
    "mask['valid'] = valid_mask\n",
    "mask['test'] = test_mask\n",
    "# batch shape: torch.Size([1, 6163, 26, 10])\n",
    "print(valid_data.shape)\n",
    "print(valid_mask[10, :, :])\n",
    "print(valid_target[10, :, :])\n",
    "\n",
    "print(valid_target[10, :, :].masked_select(valid_mask[0, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now start training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False, used: False\n"
     ]
    }
   ],
   "source": [
    "from test_tube import Experiment \n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "\n",
    "# PyTorch summarywriter with a few bells and whistles    \n",
    "exp = Experiment(save_dir=os.getcwd())\n",
    "\n",
    "# pass in experiment for automatic tensorboard logging.    \n",
    "trainer = Trainer(experiment=exp, max_nb_epochs=30, train_percent_check=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 65/111 [00:39<05:22,  7.01s/it, avg_val_loss=1.51, batch_nb=62, epoch=5, tng_loss=1.619, v_nb=29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Name        Type  Params\n",
      "0      gc_lstm  GCLSTMCell    6318\n",
      "1  gc_lstm.x2h      Linear    2808\n",
      "2  gc_lstm.h2h      Linear    2808\n",
      "3           fc      Linear      27\n",
      "4      dropout     Dropout       0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 26/111 [00:06<00:22,  3.79it/s, avg_val_loss=0.821, batch_nb=24, epoch=23, tng_loss=0.859, v_nb=29]]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-f214c2a0fc59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m model = TGCN(input_dim=26, hidden_dim=26, layer_dim=2, output_dim=1, adj=adj, \n\u001b[1;32m      9\u001b[0m              datasets=data, mask=mask)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_tng_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36mrun_tng_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__run_tng_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0mearly_stop_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_result\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__run_tng_batch\u001b[0;34m(self, data_batch, batch_nb)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_specific_tqdm_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tng_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;31m# track metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pytorch_lightning/models/trainer.py\u001b[0m in \u001b[0;36m__tng_forward\u001b[0;34m(self, data_batch, batch_nb, opt_idx)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/speed-imputation/src/tgcn/temporal_spatial_model.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/speed-imputation/src/tgcn/temporal_spatial_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgc_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/speed-imputation/src/tgcn/layers/lstmcell.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hx, cx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import imp  \n",
    "import src.tgcn.temporal_spatial_model as l\n",
    "import src.tgcn.layers.lstmcell as h\n",
    "h = imp.reload(h)\n",
    "\n",
    "l = imp.reload(l)\n",
    "TGCN = l.TGCN\n",
    "model = TGCN(input_dim=26, hidden_dim=26, layer_dim=2, output_dim=1, adj=adj, \n",
    "             datasets=data, mask=mask)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
